{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1c988-8af5-43f3-8a4c-b4b7f26ca4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import editdistance\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import trange\n",
    "from bert_score import BERTScorer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e7bcf-71d6-4ddd-85f7-08bfac00a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(results, config, average='macro'):\n",
    "    actuals, predictions, correctness = [], [], []\n",
    "    for i in range(len(results['input'])):\n",
    "        input, answer, actual = results['input'][i], results['prediction'][i], results['target'][i]\n",
    "        options = [f'\\\"{option}\\\"' for option in results['options'][i].split('\\t')]\n",
    "        if config['filename'] in ['english_proverbs/task.json', 'tracking_shuffled_objects/task.json', 'logical_deduction/task.json']:\n",
    "            options = ['\\\"{}\\\"'.format(option.rstrip('.')) for option in results['options'][i].split('\\t')]\n",
    "        if config['filename'] in ['modified_arithmetic/task.json']:\n",
    "            input = input.replace(' ?', '')\n",
    "            answer = answer.replace('?', '').replace(input, '')\n",
    "        if config['model'] in [\n",
    "            'chavinlo/alpaca-native',\n",
    "            'chavinlo/alpaca-13b',\n",
    "            'models--llama/7B',\n",
    "            'models--llama/13B',\n",
    "            'models--llama/30B'\n",
    "        ]:\n",
    "            answer = answer.lstrip('unk>').replace('<unk>', '').replace('<s>', '').replace('</s>', '').replace('⁇', '')\n",
    "        answer = answer.lstrip(',').lstrip('.').lstrip(' ').lstrip('\\n').lower()\n",
    "        prediction = '-100'\n",
    "        if config['prompt_type'] in ['closed', 'open', 'closed-info_removed']:\n",
    "            for option in options:\n",
    "                bare = option.lstrip('\\\"').rstrip('\\\"') # some models did not output quotation marks\n",
    "                if prediction == '-100':\n",
    "                    if option.lower() == answer[:len(option)] or bare.lower() == answer[:len(bare)]:\n",
    "                        prediction = option\n",
    "                elif option.lower() in answer[len(option):] or bare.lower() in answer[len(bare):]:\n",
    "                    prediction = '-100'\n",
    "                    break\n",
    "        if config['prompt_type'] in ['closed-adv']:\n",
    "            if (actual == answer[:len(actual)] and\n",
    "                '(a)' not in answer[len(actual):] and\n",
    "                '(b)' not in answer[len(actual):] and\n",
    "                '(c)' not in answer[len(actual):]):\n",
    "                prediction = actual\n",
    "        actuals.append(actual)\n",
    "        predictions.append(prediction)\n",
    "        if actual == prediction:\n",
    "            correctness.append('T')\n",
    "        else:\n",
    "            correctness.append('F')\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actuals, predictions, average=average, zero_division=0)\n",
    "    results['exact_match_correctness'] = correctness\n",
    "    with open('results/results-{}.json'.format(config['run_id']), 'w') as file:\n",
    "        json.dump(results, file, indent=4, ensure_ascii=False)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def bert_score(results, config, scorer, average='macro'):\n",
    "    actuals, predictions, correctness = [], [], []\n",
    "    for i in trange(len(results['input'])):\n",
    "        actual = results['target'][i]\n",
    "        options = [f'\\\"{option}\\\"' for option in results['options'][i].split('\\t')]\n",
    "        if config['filename'] in ['english_proverbs/task.json', 'tracking_shuffled_objects/task.json', 'logical_deduction/task.json']:\n",
    "            options = ['\\\"{}\\\"'.format(option.rstrip('.')) for option in results['options'][i].split('\\t')]\n",
    "        _, _, f1s = scorer.score([results['prediction'][i] for _ in range(len(options))], options)\n",
    "        actuals.append(actual)\n",
    "        predictions.append(options[f1s.argmax()])\n",
    "        if actual == options[f1s.argmax()]:\n",
    "            correctness.append('T')\n",
    "        else:\n",
    "            correctness.append('F')\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actuals, predictions, average=average, zero_division=0)\n",
    "    results['bert_score_correctness'] = correctness\n",
    "    with open('results/results-{}.json'.format(config['run_id']), 'w') as file:\n",
    "        json.dump(results, file, indent=4, ensure_ascii=False)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def average_edit_distance(results, config):\n",
    "    distances = 0\n",
    "    for i in range(len(results['input'])):\n",
    "        actual = results['target'][i]\n",
    "        if config['filename'] in ['modified_arithmetic/task.json']:\n",
    "            input = results['input'][i].replace(' ?', '')\n",
    "            prediction = results['prediction'][i].replace('?', '').replace(input, '')\n",
    "        if config['model'] in [\n",
    "            'chavinlo/alpaca-native',\n",
    "            'chavinlo/alpaca-13b',\n",
    "            'models--llama/7B',\n",
    "            'models--llama/13B',\n",
    "            'models--llama/30B'\n",
    "        ]:\n",
    "            prediction = results['prediction'][i].lstrip('unk>').lstrip('?? ').replace('<unk>', '').replace('<s>', '').replace('</s>', '')\n",
    "        else:\n",
    "            prediction = results['prediction'][i]\n",
    "        distances += editdistance.eval(actual, prediction)\n",
    "    return distances / len(results['input'])\n",
    "\n",
    "def average_single_bert_score(results, scorer, config):\n",
    "    predictions = results['prediction']\n",
    "    if config['filename'] in ['modified_arithmetic/task.json']:\n",
    "        for i in range(len(results['input'])):\n",
    "            input = results['input'][i].replace(' ?', '')\n",
    "            predictions[i] = results['prediction'][i].replace('?', '').replace(input, '')\n",
    "    if config['model'] in [\n",
    "            'chavinlo/alpaca-native',\n",
    "            'chavinlo/alpaca-13b',\n",
    "            'models--llama/7B',\n",
    "            'models--llama/13B',\n",
    "            'models--llama/30B'\n",
    "        ]:\n",
    "        for i in range(len(results['input'])):\n",
    "            predictions[i] = results['prediction'][i].lstrip('unk>').lstrip('?? ').replace('<unk>', '').replace('<s>', '').replace('</s>', '')\n",
    "    _, _, f1s = scorer.score(predictions, results['target'])\n",
    "    return float(f1s.mean())\n",
    "    \n",
    "def loss_and_gain(results, config, evaluation_scores_df, metric):\n",
    "    try:\n",
    "        run_id_reference = evaluation_scores_df['run_id'][\n",
    "            (evaluation_scores_df.model==pairs[config['model']]) &\n",
    "            (evaluation_scores_df.prompt_type==config['prompt_type']) &\n",
    "            (evaluation_scores_df.number_of_shots==config['number_of_shots']) &\n",
    "            (evaluation_scores_df.seed==config['seed'])\n",
    "        ].tolist()[0]\n",
    "    except Exception as e:\n",
    "        run_id_reference = evaluation_scores_df['run_id'][\n",
    "            (evaluation_scores_df.model==pairs[config['model']]) &\n",
    "            (evaluation_scores_df.prompt_type==config['prompt_type']) &\n",
    "            (evaluation_scores_df.number_of_shots==config['number_of_shots']) &\n",
    "            (evaluation_scores_df.seed==str(config['seed']))\n",
    "        ].tolist()[0]\n",
    "    try:\n",
    "        with open(f'results/results-{run_id_reference}.json') as file:\n",
    "            results_reference = json.loads(file.read())\n",
    "    except Exception as e:\n",
    "        with open(f'results_20230618/results-{run_id_reference}.json') as file:\n",
    "            results_reference = json.loads(file.read())\n",
    "    loss, gain = 0, 0\n",
    "    for i in range(len(results['input'])):\n",
    "        correctness = results[metric][i]\n",
    "        correctness_reference = results_reference[metric][i]\n",
    "        # candidate model got it wrong when reference model got it right\n",
    "        if correctness == 'F' and correctness_reference == 'T':\n",
    "            loss += 1\n",
    "        # candidate model got it right when reference model got it wrong\n",
    "        if correctness == 'T' and correctness_reference == 'F':\n",
    "            gain += 1\n",
    "    return loss, gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57e30f-70f9-46f3-9c62-5fe86db9a39d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e298a3a-6879-4fd7-90af-85961d0a3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_df = pd.read_csv('evaluation_scores.csv')\n",
    "scorer = BERTScorer(model_type='roberta-large', device='cuda')\n",
    "\n",
    "for run_id in config_df.run_id.tolist():\n",
    "    config = {k: v[0] for k, v in config_df[config_df.run_id == run_id].to_dict(orient='list').items()}\n",
    "    name = config['filename'].replace('/task.json', '')\n",
    "    with open(f'results/results-{run_id}.json') as file:\n",
    "        results = json.loads(file.read())\n",
    "    config['exact_match_accuracy'], _, _, config['exact_match_f1'] = exact_match(results, config)\n",
    "    config['bert_score_accuracy'], config['bert_score_f1'] = -100, -100\n",
    "    if config['prompt_type'] != 'closed-adv' and name not in ['gsm8k', 'codenames', 'modified_arithmetic']:\n",
    "        config['bert_score_accuracy'], _, _, config['bert_score_f1'] = bert_score(results, config, scorer)\n",
    "    with open(f'evaluation_scores/evaluation_scores_{name}.txt', 'a') as file:\n",
    "        file.write(('\\t'.join(['{' + i + '}' for i in config.keys()]) + '\\n').format(**config))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644e513-f236-47c7-ac5e-0bda1b66a39c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec22a1-8094-4950-8799-8319360a3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('evaluation_scores/'):\n",
    "    config_df = pd.read_csv(\n",
    "        'evaluation_scores/' + filename,\n",
    "        sep='\\t',\n",
    "        names=['run_id', 'filename', 'number_of_data',\n",
    "                'model', 'prompt_type', 'number_of_shots',\n",
    "                'temperature', 'max_new_tokens', 'batch_size',\n",
    "                'pad_token', 'pad_token_id', 'eos_token_id',\n",
    "                'seed', 'device', 'exact_match_accuracy',\n",
    "                'exact_match_f1', 'bert_score_accuracy',\n",
    "                'bert_score_f1']\n",
    "    )\n",
    "    for run_id in config_df.run_id.tolist():\n",
    "        config = {k: v[0] for k, v in config_df[config_df.run_id == run_id].to_dict(orient='list').items()}\n",
    "        with open(f'results/results-{run_id}.json') as file:\n",
    "            results = json.loads(file.read())\n",
    "        config['edit_distance'] = average_edit_distance(results, config)\n",
    "        with open(f'evaluation_scores/plus edit distance/plus_edit_distance.txt', 'a') as file:\n",
    "            file.write(('\\t'.join(['{' + i + '}' for i in config.keys()]) + '\\n').format(**config))\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c73317-d1b9-4a62-b90d-74bf27ab2675",
   "metadata": {
    "tags": []
   },
   "source": [
    "## single bert score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6da38-eb0f-47c3-a433-cd686d8c5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BERTScorer(model_type='roberta-large', device='cuda')\n",
    "config_df = pd.read_csv(\n",
    "    'evaluation_scores/plus edit distance/plus_edit_distance.txt',\n",
    "    sep='\\t'\n",
    ")\n",
    "for run_id in config_df.run_id.tolist():\n",
    "    config = {k: v[0] for k, v in config_df[config_df.run_id == run_id].to_dict(orient='list').items()}\n",
    "    config['single_bert_score'] = -100\n",
    "    with open(f'results/results-{run_id}.json') as file:\n",
    "        results = json.loads(file.read())\n",
    "    config['single_bert_score'] = average_single_bert_score(results, scorer, config)\n",
    "    with open(f'evaluation_scores/plus 2/plus_2.txt', 'a') as file:\n",
    "        file.write(('\\t'.join(['{' + i + '}' for i in config.keys()]) + '\\n').format(**config))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69daf6d-6a55-4cf4-9300-abce2cc2d199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
